\section{Introduction}

Most of us wonder how platforms like Facebook and YouTube choose what to recommend. The lack of transparency in these recommendation systems often degrades user confidence, recommendation acceptance, and user experience~\cite{Tintarev2015}. To address these concerns, there's a rising research focus on making recommendation systems clearer and more understandable~\cite{Papadimitriou2012, Tintarev2015, Zhang2020}. Some systems, such as PageRank~\cite{haveliwala2003personalizedpagerank} and RippleNet~\cite{wang2018ripplenet} provide a graph for making the recommendation. Hence, the explanation is the path between the recommended items and the seed items. However, the average user may be unfamiliar with a graph representation.

Yet, everyone is familiar with a textual explanation. Therefore, a popular alternative is to convert graph-based explanations into textual explanations using a template and an algorithm, which we refer to as “template-based” explanations. Still, these explanations can sometimes be perceived as impersonal or uninspiring. A promising approach is to use large language models (LLMs) to generate\footnote{In the context of this abstract, when we refer to the generation of LLMs-based recommendations explanations, we actually mean allowing an LLMs to rephrase an explanation or interpret a graph representation of an explainable recommendation.} textual explanations for recommendations. The generated text is typically well-written and clear, making it easy for humans to understand\cite{bommasani2021foundationModels}.

Motivated by these perspectives, we put them to the test in the generation of explanations for recommendations during the TRAIL’23 Workshop\footnote{\url{https://trail.ac/en/trail-summer-workshops/the-trail-summer-workshop-2023/}, more details in the Appendix}. Concretely, we defined two goals to address during the workshop.
% technical goal: implement working examples and gain experience
The first is implementing working examples of recommendation explanations generated with LLMs using various recommendation methods and LLM models. This way, we could assess the technical possibilities and limitations of LLMs.
%The experience gained through this implementation will be translated into a cookbook with implementation tips and solutions to technical issues, and experimentation notebooks with working examples.
% evaluation goal: build a picture of the potential of LLMs w.r.t. the chosen use case
The second goal is to evaluate explanations generated by different LLM models and recommendation methods to understand their qualities and their limitations in this context. To achieve this goal, we designed a user-based evaluation method to assess explanations w.r.t. different explanatory goals and subjective properties~\cite{Tintarev2015}.
%The understanding gained through those evaluations will be valuable and complementary to the first goal.
