\section{Introduction}

Most of us wonder daily why platforms like Facebook and YouTube recommend specific people or videos to us. The lack of transparency in these recommendations often leaves us without a clear explanation. This can degrade user confidence, recommendation acceptance and, more generally, the user experience~\cite{Tintarev2015}. To address those important concerns, a growing field of research focuses on making recommendation systems more transparent and explainable~\cite{Papadimitriou2012, Tintarev2015, Zhang2020}. A promising approach is to use large language models (LLMs) to generate explanations\footnote{In the context of this abstract, when we refer to the generation of LLMs-based recommendations explanations, we actually mean using an LLMs to rephrase an explanation or interpret a graph representation of an explainable recommendation.} for recommendations. LLMs are initially pre-trained on extensive corpora, allowing them to perform a versatile range of natural language processing (NLP) tasks\cite{bommasani2021foundationModels}. The generated text is typically well-written and clear, making it easy for humans to understand.

Motivated by these perspectives, we put them to the test in the generation of explanations for recommendations during the TRAILâ€™23 Workshop\footnote{\url{https://trail.ac/en/trail-summer-workshops/the-trail-summer-workshop-2023/}, more details in the Appendix}. Concretely, we defined two goals to address during the workshop.
% technical goal: implement working examples and gain experience
The first is implementing working examples of recommendation explanations generated with LLMs using various recommendation methods and LLM models. This way, we could assess the technical possibilities and limitations of LLMs.
%The experience gained through this implementation will be translated into a cookbook with implementation tips and solutions to technical issues, and experimentation notebooks with working examples.
% evaluation goal: build a picture of the potential of LLMs w.r.t. the chosen use case
The second goal is to evaluate explanations generated by different LLM models and recommendation methods to understand their qualities and their limitations in this context. To achieve this goal, we designed a user-based evaluation method to assess explanations w.r.t. different explanatory goals and subjective properties~\cite{Tintarev2015}.
%The understanding gained through those evaluations will be valuable and complementary to the first goal.
