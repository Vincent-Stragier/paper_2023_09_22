@article{hoffman2023measures,
  title={Measures for explainable AI: Explanation goodness, user satisfaction, mental models, curiosity, trust, and human-AI performance},
  author={Hoffman, Robert R and Mueller, Shane T and Klein, Gary and Litman, Jordan},
  journal={Frontiers in Computer Science},
  volume={5},
  pages={1096257},
  year={2023},
  publisher={Frontiers Media SA}
}

@article{shin2021effects,
  title={The effects of explainability and causability on perception, trust, and acceptance: Implications for explainable AI},
  author={Shin, Donghee},
  journal={International Journal of Human-Computer Studies},
  volume={146},
  pages={102551},
  year={2021},
  publisher={Elsevier}
}

@incollection{tintarev2015explaining,
  title={Explaining recommendations: Design and evaluation},
  author={Tintarev, Nava and Masthoff, Judith},
  booktitle={Recommender systems handbook},
  pages={353--382},
  year={2015},
  publisher={Springer}
}

@article{zhang2020explainable,
  title={Explainable recommendation: A survey and new perspectives},
  author={Zhang, Yongfeng and Chen, Xu and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={14},
  number={1},
  pages={1--101},
  year={2020},
  publisher={Now Publishers, Inc.}
}

@inproceedings{kunkel2019let,
  title={Let me explain: Impact of personal and impersonal explanations on trust in recommender systems},
  author={Kunkel, Johannes and Donkers, Tim and Michael, Lisa and Barbu, Catalin-Mihai and Ziegler, J{\"u}rgen},
  booktitle={Proceedings of the 2019 CHI conference on human factors in computing systems},
  pages={1--12},
  year={2019}
}

@inproceedings{guesmi2022explaining,
  title={Explaining user models with different levels of detail for transparent recommendation: A user study},
  author={Guesmi, Mouadh and Chatti, Mohamed Amine and Vorgerd, Laura and Ngo, Thao and Joarder, Shoeb and Ain, Qurat Ul and Muslim, Arham},
  booktitle={Adjunct Proceedings of the 30th ACM Conference on User Modeling, Adaptation and Personalization},
  pages={175--183},
  year={2022}
}

@inproceedings{guesmi2021input,
  title={Input or Output: Effects of Explanation Focus on the Perception of Explainable Recommendation with Varying Level of Details.},
  author={Guesmi, Mouadh and Chatti, Mohamed Amine and Vorgerd, Laura and Joarder, Shoeb Ahmed and Ain, Qurat Ul and Ngo, Thao and Zumor, Shadi and Sun, Yiqi and Ji, Fangzheng and Muslim, Arham},
  booktitle={IntRS@ RecSys},
  pages={55--72},
  year={2021}
}

@article{gedikli2014should,
  title={How should I explain? A comparison of different explanation types for recommender systems},
  author={Gedikli, Fatih and Jannach, Dietmar and Ge, Mouzhi},
  journal={International Journal of Human-Computer Studies},
  volume={72},
  number={4},
  pages={367--382},
  year={2014},
  publisher={Elsevier}
}

@article{donoso2023towards,
  title={Towards a Comprehensive Human-Centred Evaluation Framework for Explainable AI},
  author={Donoso-Guzm{\'a}n, Ivania and Ooge, Jeroen and Parra, Denis and Verbert, Katrien},
  journal={arXiv preprint arXiv:2308.06274},
  year={2023}
}

@inproceedings{dominguez2019effect,
  title={The effect of explanations and algorithmic accuracy on visual recommender systems of artistic images},
  author={Dominguez, Vicente and Messina, Pablo and Donoso-Guzm{\'a}n, Ivania and Parra, Denis},
  booktitle={Proceedings of the 24th International Conference on Intelligent User Interfaces},
  pages={408--416},
  year={2019}
}

@article{chen2022measuring,
  title={Measuring" why" in recommender systems: A comprehensive survey on the evaluation of explainable recommendation},
  author={Chen, Xu and Zhang, Yongfeng and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2202.06466},
  year={2022}
}

@inproceedings{balog2020measuring,
  title={Measuring recommendation explanation quality: The conflicting goals of explanations},
  author={Balog, Krisztian and Radlinski, Filip},
  booktitle={Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval},
  pages={329--338},
  year={2020}
}

@inproceedings{szymanski2021visual,
  title={Visual, textual or hybrid: the effect of user expertise on different explanations},
  author={Szymanski, Maxwell and Millecamp, Martijn and Verbert, Katrien},
  booktitle={26th International Conference on Intelligent User Interfaces},
  pages={109--119},
  year={2021}
}

@article{schwartz2023enhancing,
  title={Enhancing Trust in LLM-Based AI Automation Agents: New Considerations and Future Challenges},
  author={Schwartz, Sivan and Yaeli, Avi and Shlomov, Segev},
  journal={arXiv preprint arXiv:2308.05391},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{devlin2018bert,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.04805},
  eprinttype = {arXiv},
  eprint    = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{bommasani2021foundationModels,
  title={On the Opportunities and Risks of Foundation Models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv e-prints},
  pages={arXiv--2108},
  year={2021}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{weichain,
  title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed H and Le, Quoc V and Zhou, Denny and others},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@inproceedings{reynolds2021prompt,
  title={Prompt programming for large language models: Beyond the few-shot paradigm},
  author={Reynolds, Laria and McDonell, Kyle},
  booktitle={Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--7},
  year={2021}
}

@article{weiemergent,
  title={Emergent Abilities of Large Language Models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={Transactions on Machine Learning Research},
  year={2022}
}

@article{shinn2023reflexion,
  title={Reflexion: an autonomous agent with dynamic memory and self-reflection},
  author={Shinn, Noah and Labash, Beck and Gopinath, Ashwin},
  journal={arXiv e-prints},
  pages={arXiv--2303},
  year={2023}
}

@article{bubeck2023sparks,
  title={Sparks of Artificial General Intelligence: Early experiments with GPT-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv e-prints},
  pages={arXiv--2303},
  year={2023}
}

@incollection{DeGemmis2017,
	address = {New York, NY},
	archivePrefix = {arXiv},
	arxivId = {1605.00069},
	author = {de Gemmis, Marco and Lops, Pasquale and Polignano, Marco},
	booktitle = {Encyclopedia of Social Network Analysis and Mining},
	doi = {10.1007/978-1-4614-7163-9_110158-1},
	eprint = {1605.00069},
	isbn = {978-1-4614-7163-9},
	pages = {1--13},
	publisher = {Springer New York},
	title = {{Recommender Systems, Basics Of}},
	year = {2017}
}

@incollection{Ricci2017,
	address = {New York, NY},
	archivePrefix = {arXiv},
	arxivId = {1605.00069},
	author = {Ricci, Francesco},
	booktitle = {Encyclopedia of Social Network Analysis and Mining},
	doi = {10.1007/978-1-4614-7163-9_88-1},
	eprint = {1605.00069},
	isbn = {978-1-4614-7163-9},
	pages = {1--12},
	publisher = {Springer New York},
	title = {{Recommender Systems: Models and Techniques}},
	year = {2017}
}

@book{Aggarwal2016,
	author = {Charu C. Aggarwal},
	title = {Recommender Systems : The Textbook},
	year = {2016},
	isbn = {978-3-319-80619-8},
	doi = {10.1007/978-3-319-29659-3},
	edition = {1st},
	publisher = {Springer International Publishing},
}

@article{Papadimitriou2012,
	author = {Papadimitriou, Alexis and Symeonidis, Panagiotis and Manolopoulos, Yannis},
	doi = {10.1007/s10618-011-0215-0},
	issn = {1384-5810},
	journal = {Data Mining and Knowledge Discovery},
	month = {may},
	number = {3},
	pages = {555--583},
	title = {{A generalized taxonomy of explanations styles for traditional and social recommender systems}},
	url = {http://link.springer.com/10.1007/s10618-011-0215-0},
	volume = {24},
	year = {2012}
}

@incollection{Tintarev2015,
	address = {Boston, MA},
	author = {Tintarev, Nava and Masthoff, Judith},
	booktitle = {Recommender Systems Handbook},
	doi = {10.1007/978-1-4899-7637-6_10},
	pages = {353--382},
	publisher = {Springer US},
	title = {{Explaining Recommendations: Design and Evaluation}},
	year = {2015}
}

@article{Zhang2020,
	archivePrefix = {arXiv},
	arxivId = {1804.11192},
	author = {Zhang, Yongfeng and Chen, Xu},
	doi = {10.1561/1500000066},
	eprint = {1804.11192},
	issn = {1554-0669},
	journal = {Foundations and Trends{\textregistered} in Information Retrieval},
	number = {1},
	pages = {1--101},
	title = {{Explainable Recommendation: A Survey and New Perspectives}},
	url = {http://www.nowpublishers.com/article/Details/INR-066},
	volume = {14},
	year = {2020}
}

@article{chiang2023can,
  title={Can Large Language Models Be an Alternative to Human Evaluations?},
  author={Chiang, Cheng-Han and Lee, Hung-Yi},
  journal={arXiv e-prints},
  pages={arXiv--2305},
  year={2023}
}

@misc{xu2023wizardlm,
      title={WizardLM: Empowering Large Language Models to Follow Complex Instructions}, 
      author={Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang},
      year={2023},
      eprint={2304.12244},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{taori2023alpaca,
  title={Stanford alpaca: An instruction-following llama model},
  author={Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  journal={GitHub repository},
  year={2023}
}

@misc{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  year={2023},
  publisher={March}
}

@article{hsieh2023distilling,
  title={Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes},
  author={Hsieh, Cheng-Yu and Li, Chun-Liang and Yeh, Chih-Kuan and Nakhost, Hootan and Fujii, Yasuhisa and Ratner, Alexander and Krishna, Ranjay and Lee, Chen-Yu and Pfister, Tomas},
  journal={arXiv preprint arXiv:2305.02301},
  year={2023}
}

@article{Li2023PersonalizedPromptLearningExplainableRecommendation,
  title = {Personalized {{Prompt Learning}} for {{Explainable Recommendation}}},
  author = {Li, Lei and Zhang, Yongfeng and Chen, Li},
  year = {2023},
  month = mar,
  journal = {ACM Transactions on Information Systems},
  volume = {41},
  number = {4},
  pages = {103:1--103:26},
  issn = {1046-8188},
  doi = {10.1145/3580488},
  url = {https://dl.acm.org/doi/10.1145/3580488},
  urldate = {2023-05-11},
  abstract = {Providing user-understandable explanations to justify recommendations could help users better understand the recommended items, increase the system's ease of use, and gain users' trust. A typical approach to realize it is natural language generation. However, previous works mostly adopt recurrent neural networks to meet the ends, leaving the potentially more effective pre-trained Transformer models under-explored. In fact, user and item IDs, as important identifiers in recommender systems, are inherently in different semantic space as words that pre-trained models were already trained on. Thus, how to effectively fuse IDs into such models becomes a critical issue. Inspired by recent advancement in prompt learning, we come up with two solutions: find alternative words to represent IDs (called discrete prompt learning) and directly input ID vectors to a pre-trained model (termed continuous prompt learning). In the latter case, ID vectors are randomly initialized but the model is trained in advance on large corpora, so they are actually in different learning stages. To bridge the gap, we further propose two training strategies: sequential tuning and recommendation as regularization. Extensive experiments show that our continuous prompt learning approach equipped with the training strategies consistently outperforms strong baselines on three datasets of explainable recommendation.},
  keywords = {Explainable recommendation,pre-trained language model,prompt learning,Transformer},
  file = {/home/julien/Desktop/Zotero/PDFs/Li2023PersonalizedPromptLearningExplainableRecommendation.pdf}
}

@inproceedings{Kokubo2022ExplainableRecommendationEnhancingReviewProperties,
  title = {Explainable {{Recommendation Enhancing Review Properties}} and {{PPLM}}},
  booktitle = {2022 {{IEEE}}/{{WIC}}/{{ACM International Joint Conference}} on {{Web Intelligence}} and {{Intelligent Agent Technology}} ({{WI-IAT}})},
  author = {Kokubo, Akihiro and Sugiyama, Kazunari},
  year = {2022},
  month = nov,
  pages = {151--158},
  doi = {10.1109/WI-IAT55865.2022.00030},
  abstract = {Explainable recommendation, which provides items and explanation why they are recommended, have attracted a lot of attention as it could improve transparency, persuasiveness, effectiveness, reliability, and user satisfaction of recommender systems. A lot of explainable recommender systems have been proposed so far. However, they do not take review properties into account, need to further improve generation of explanation, and also need to analyze specific cases in recommendation.Our work aims at developing an explainable recommender model that improves quality of generated explanations as well as accuracy of recommendation. To achieve this, we propose an end-to-end architecture that leverages properties of review sentences and Plug and Play Language Model (PPLM). Experimental results on publicly available datasets demonstrate that our proposed recommender model improves both accuracy of rating prediction and quality of generated explanations by employing multi-task learning framework.},
  keywords = {Analytical models,Explainability,Intelligent agents,Multitasking,Predictive models,Recommender system,Reliability,Sentence generation,Task analysis,Training},
  file = {/home/julien/Desktop/Zotero/PDFs/Kokubo2022ExplainableRecommendationEnhancingReviewProperties.pdf;/home/julien/Desktop/Zotero/PDFs/Kokubo2022ExplainableRecommendationEnhancingReviewProperties.html}
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@inproceedings{wang2018ripplenet,
    author = {Wang, Hongwei and Zhang, Fuzheng and Wang, Jialin and Zhao, Miao and Li, Wenjie and Xie, Xing and Guo, Minyi},
    title = {RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems},
    year = {2018},
    isbn = {9781450360142},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3269206.3271739},
    doi = {10.1145/3269206.3271739},
    booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
    pages = {417â€“426},
    numpages = {10},
    keywords = {knowledge graph, recommender systems, preference propagation},
    location = {Torino, Italy},
    series = {CIKM '18}
}

@article{haveliwala2003personalizedpagerank,
  author={Haveliwala, T.H.},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Topic-sensitive PageRank: a context-sensitive ranking algorithm for Web search}, 
  year={2003},
  volume={15},
  number={4},
  pages={784-796},
  doi={10.1109/TKDE.2003.1208999}
}

@article{liu2023chatgpt,
  title={Is chatgpt a good recommender? a preliminary study},
  author={Liu, Junling and Liu, Chao and Lv, Renjie and Zhou, Kang and Zhang, Yan},
  journal={arXiv preprint arXiv:2304.10149},
  year={2023}
}

@misc{yangHarnessingPowerLLMs2023a,
  title = {Harnessing the {{Power}} of {{LLMs}} in {{Practice}}: {{A Survey}} on {{ChatGPT}} and {{Beyond}}},
  shorttitle = {Harnessing the {{Power}} of {{LLMs}} in {{Practice}}},
  author = {Yang, Jingfeng and Jin, Hongye and Tang, Ruixiang and Han, Xiaotian and Feng, Qizhang and Jiang, Haoming and Yin, Bing and Hu, Xia},
  year = {2023},
  month = apr,
  number = {arXiv:2304.13712},
  eprint = {2304.13712},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-08-07},
  abstract = {This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. Firstly, we offer an introduction and brief summary of current GPT- and BERT-style LLMs. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks.We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at \textbackslash url\{https://github.com/Mooler0410/LLMsPracticalGuide\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{touvronLlamaOpenFoundation2023,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  year = {2023},
  month = jul,
  number = {arXiv:2307.09288},
  eprint = {2307.09288},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-08-03},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{radfordImprovingLanguageUnderstanding,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  langid = {english},
  year = {2018}
}

@article{ji2023hallucination,
author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
title = {Survey of Hallucination in Natural Language Generation},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3571730},
doi = {10.1145/3571730},
abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before.In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {248},
numpages = {38},
keywords = {extrinsic hallucination, consistency in NLG, Hallucination, faithfulness in NLG, intrinsic hallucination, factuality in NLG}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}
